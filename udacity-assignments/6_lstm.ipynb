{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "data_root = './data'\n",
    "def maybe_download(fname, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  dst_fname = os.path.join(data_root, fname)\n",
    "  if not os.path.exists(dst_fname):\n",
    "    dst_fname, _ = urlretrieve(url + fname, dst_fname)\n",
    "  statinfo = os.stat(dst_fname)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found/verified %s: Stored it in %s' % (fname, dst_fname))\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + fname + '. Can you get to it with a browser?')\n",
    "  return dst_fname\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(text[:100])\n",
    "print([''] * 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('Ã¯'))\n",
    "print(\"'%c', '%c', '%c'\"%(id2char(1), id2char(26), id2char(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  # Cross Entropy: avoid \"nan\" faults by lower bounding prob to 1e-10\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (row [[a, b, c]]) of prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random row of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with tf.device('/cpu:0'), graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input (W: Wx), previous output (U: Uh), and bias (b).\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  # Above LSTM cell involves 4 matrix multiplications with the input, and\n",
    "  # 4 matrix multiplications with the output.\n",
    "  # Using a single matrix multiply for each, and variables that are 4 times larger\n",
    "  def lstm_cell(e, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    eo = tf.concat(1, [e, o])\n",
    "    ip_fo_op_up_xm = tf.concat(0,\n",
    "        [tf.concat(1, [ix, fx, cx, ox]), tf.concat(1, [im, fm, cm, om])])\n",
    "    ip_fo_op_up = tf.matmul(eo, ip_fo_op_up_xm) + tf.concat(1, [ib, fb, cb, ob])\n",
    "    ip_fo_op, update = tf.split_v(value=ip_fo_op_up, size_splits=[3*num_nodes, num_nodes], split_dim=1)\n",
    "    input_gate, forget_gate, output_gate = tf.split(split_dim=1, num_split=3, value=tf.sigmoid(ip_fo_op))\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(0, train_labels), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.device('/cpu:0'), tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Validation set perplexity: 4.46\n",
    "Rewrote 4 matrix multiplications for input/output to 1 matrix multiplication for input/output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class BatchIdGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=[self._batch_size], dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b] = char2id(self._text[self._cursor[b]])\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def random_bi_distribution():\n",
    "  \"\"\" Generate a random row of bi-char probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0,1.0,size=[1,vocabulary_size*vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "def bi_char_batches(batches):\n",
    "  \"\"\" Generate a corresponding batch of bigram characters from\n",
    "  a batch of unigram characters \"\"\"\n",
    "  num_batches = len(batches)\n",
    "  assert num_batches > 0\n",
    "  batch_prev = np.zeros(batches[0].shape, dtype=np.int32)\n",
    "  bi_batches =[]\n",
    "  for idx, batch in enumerate(batches):\n",
    "    b = batch_prev*vocabulary_size + batch\n",
    "    bi_batches.append(b)\n",
    "    batch_prev = batch\n",
    "  return bi_batches\n",
    "\n",
    "def ids2BiId(id1, id2):\n",
    "  return id1*vocabulary_size + id2\n",
    "\n",
    "def biId2Ids(bi_id):\n",
    "  return bi_id//vocabulary_size, bi_id%vocabulary_size\n",
    "\n",
    "def biId2bichar(bi_id):\n",
    "  id1, id2 = biId2Ids(bi_id)\n",
    "  return id2char(id1) + id2char(id2)\n",
    "\n",
    "def bichar2BiId(ch1, ch2):\n",
    "  return ids2BiId(char2id(ch1), char2id(ch2))\n",
    "\n",
    "def bi_chars(batch):\n",
    "  assert batch.ndim == 1\n",
    "  res = [biId2bichar(id) for id in batch]\n",
    "  return res\n",
    "\n",
    "def bi_batches2string(bi_batches):\n",
    "  \"\"\"Convert a sequence of bi_batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * len(bi_batches[0])\n",
    "  for b in bi_batches:\n",
    "    one_char = [c[1:] for c in bi_chars(b)]\n",
    "    s = [''.join(x) for x in zip(s, one_char)]\n",
    "  return s\n",
    "\n",
    "train_idbatches = BatchIdGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_idbatches = BatchIdGenerator(valid_text, 1, 1)\n",
    "print(bi_batches2string(bi_char_batches(train_idbatches.next())))\n",
    "print(bi_batches2string(bi_char_batches(train_idbatches.next())))\n",
    "print(bi_batches2string(bi_char_batches(valid_idbatches.next())))\n",
    "print(bi_batches2string(bi_char_batches(valid_idbatches.next())))\n",
    "print(\"ids2BiId: ch1={}, ch2={}, biId={}\".format('a', 'n', bichar2BiId('a', 'n')))\n",
    "assert biId2bichar(bichar2BiId('a','n')) == 'an'\n",
    "assert bichar2BiId(biId2bichar(41)[0], biId2bichar(41)[1]) == 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def logprob_sparse(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  # Cross Entropy: avoid \"nan\" faults by lower bounding prob to 1e-10\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  assert predictions.ndim == 2 and labels.ndim == 1\n",
    "  return np.mean(-np.log(predictions)[np.arange(labels.shape[0]), labels])\n",
    "\n",
    "tmp = np.arange(12).reshape(3,4)\n",
    "predictions = tmp/np.sqrt(np.sum(tmp*tmp, axis=1, keepdims=True))\n",
    "loss = logprob_sparse(predictions, np.array([1,2,0]))\n",
    "print(\"Loss=%.5f\"%loss)\n",
    "assert np.abs(loss - 0.93926) < 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with tf.device('/cpu:0'), graph.as_default():  \n",
    "  # Parameters:\n",
    "  # Embedding Layer:\n",
    "  # Two Characters are mapped into an embedding layer\n",
    "  # uniform_init = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "  uniform_init = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "  with tf.variable_scope(\"Embedding\", initializer=uniform_init) as scope:\n",
    "    embeds = tf.get_variable(\"Embeds\",\n",
    "                             [vocabulary_size*vocabulary_size, embedding_size], initializer=uniform_init)\n",
    "  \n",
    "  # Input gate: input (W: Wx), previous output (U: Uh), and bias (b).\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Variables saving state/output (i.e. ct/ht in paper) across unrollings:\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  # Above LSTM cell involves 4 matrix multiplications with the input, and\n",
    "  # 4 matrix multiplications with the output.\n",
    "  # Using a single matrix multiply for each, and variables that are 4 times larger\n",
    "  def lstm_cell(e, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    eo = tf.concat(1, [e, o])\n",
    "    ip_fo_op_up_xm = tf.concat(0,\n",
    "                            [tf.concat(1, [ix, fx, cx, ox]), tf.concat(1, [im, fm, cm, om])])\n",
    "    ip_fo_op_up = tf.matmul(eo, ip_fo_op_up_xm) + tf.concat(1, [ib, fb, cb, ob])\n",
    "    ip_fo_op, update = tf.split_v(value=ip_fo_op_up, size_splits=[3*num_nodes, num_nodes], split_dim=1)\n",
    "    input_gate, forget_gate, output_gate = tf.split(split_dim=1, num_split=3, value=tf.sigmoid(ip_fo_op))\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # INPUT DATA\n",
    "  # 1. Input/Labels are provided via a list of contiguous characters\n",
    "  # of length=(num_unrollings + 1)\n",
    "  train_data_ph = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data_ph.append(tf.placeholder(tf.int32, shape=[batch_size], name=\"train_data\"))\n",
    "  # 2. Dropout: Only applied on input/projection\n",
    "  # (i.e. x/y in paper and not in internal state/output ct/ht)\n",
    "  dropout_ph = tf.placeholder(tf.float32, name= \"dropout\")\n",
    "\n",
    "  # inputs are bicharId extracted from train_data\n",
    "  train_inputs = list()\n",
    "  prev_data = tf.zeros([batch_size], dtype=tf.int32)\n",
    "  for i in range(num_unrollings):\n",
    "    cur_data = train_data_ph[i]\n",
    "    # bicharId = id1*vocabulary_size + id2\n",
    "    train_inputs.append(vocabulary_size*prev_data + cur_data)\n",
    "    prev_data = cur_data\n",
    "\n",
    "  # labels are inputs shifted by one time step.\n",
    "  train_labels = train_data_ph[1:]  \n",
    "\n",
    "  # Unroll LSTM loop upto num_unrollings times.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state    = saved_state\n",
    "  for i in train_inputs:\n",
    "    # apply dropout at input/embedding layer or output layer:\n",
    "    # i.e. when layers are crossed - not in the recurrent layer\n",
    "    train_embed = tf.nn.dropout(tf.nn.embedding_lookup(embeds, i), dropout_ph)\n",
    "    output, state = lstm_cell(train_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # apply dropout at input/embedding layer or output layer:\n",
    "    # i.e. when layers are crossed - not in the recurrent layer\n",
    "    # Classifier\n",
    "    projection = tf.nn.dropout(tf.concat(0, outputs), dropout_ph)\n",
    "    logits = tf.nn.xw_plus_b(projection, w, b)\n",
    "    labels = tf.concat(0, train_labels)\n",
    "    xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      logits=logits, labels=labels)\n",
    "    loss = tf.reduce_mean(xent)\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  #  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.75, staircase=True)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1 of biCharId, no unrolling, and no dropout\n",
    "  sample_input_ph = tf.placeholder(tf.int32, shape=[1], name=\"sample_input\")\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # create a group tensor op to reset sample output/state\n",
    "  reset_sample_state_op = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "\n",
    "  sample_embed = tf.nn.embedding_lookup(embeds, sample_input_ph)\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.device('/cpu:0'), tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_idbatches.next()\n",
    "    feed_dict = { dropout_ph: 0.95 }\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data_ph[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob_sparse(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          prevCharId = 0\n",
    "          nextCharId = sample_distribution(random_distribution()[0])\n",
    "          biId = ids2BiId(prevCharId, nextCharId)\n",
    "          sentence = id2char(nextCharId)\n",
    "          reset_sample_state_op.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input_ph: [biId]})\n",
    "            prevCharId = nextCharId\n",
    "            nextCharId = sample_distribution(prediction[0])\n",
    "            biId = ids2BiId(prevCharId, nextCharId)\n",
    "            sentence += id2char(nextCharId)\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state_op.run()\n",
    "      valid_logprob = 0\n",
    "      b_prev = np.random.randint(vocabulary_size)\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_idbatches.next()\n",
    "        biId = ids2BiId(b_prev, b[0])\n",
    "        predictions = sample_prediction.eval({sample_input_ph: biId})\n",
    "        valid_logprob = valid_logprob + logprob_sparse(predictions, b[1])\n",
    "        b_prev = b[0]\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Validation set perplexity: 3.98\n",
    "num_nodes = 64; embedding_size = 128; Dropout 0.95\n",
    "Learning Rate 10.0, global_step, 5000, 0.1, staircase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Reimplement unigram using embedding as input instead of directly feeding 1-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = num_nodes\n",
    "\n",
    "graph = tf.Graph()\n",
    "with tf.device('/cpu:0'), graph.as_default():  \n",
    "  # Parameters:\n",
    "  # Embedding Layer:\n",
    "  # Two Characters are mapped into an embedding layer\n",
    "  # uniform_init = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "  uniform_init = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "  with tf.variable_scope(\"Embedding\", initializer=uniform_init) as scope:\n",
    "    embeds = tf.get_variable(\"Embeds\",\n",
    "                             [vocabulary_size, embedding_size], initializer=uniform_init)\n",
    "  \n",
    "  # Input gate: input (W: Wx), previous output (U: Uh), and bias (b).\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Variables saving state/output (i.e. ct/ht in paper) across unrollings:\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  # Above LSTM cell involves 4 matrix multiplications with the input, and\n",
    "  # 4 matrix multiplications with the output.\n",
    "  # Using a single matrix multiply for each, and variables that are 4 times larger\n",
    "  def lstm_cell(e, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    eo = tf.concat(1, [e, o])\n",
    "    ip_fo_op_up_xm = tf.concat(0,\n",
    "                            [tf.concat(1, [ix, fx, cx, ox]), tf.concat(1, [im, fm, cm, om])])\n",
    "    ip_fo_op_up = tf.matmul(eo, ip_fo_op_up_xm) + tf.concat(1, [ib, fb, cb, ob])\n",
    "    ip_fo_op, update = tf.split_v(value=ip_fo_op_up, size_splits=[3*num_nodes, num_nodes], split_dim=1)\n",
    "    input_gate, forget_gate, output_gate = tf.split(split_dim=1, num_split=3, value=tf.sigmoid(ip_fo_op))\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # INPUT DATA\n",
    "  # 1. Input/Labels are provided via a list of contiguous characters\n",
    "  # of length=(num_unrollings + 1)\n",
    "  train_data_ph = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data_ph.append(tf.placeholder(tf.int32, shape=[batch_size], name=\"train_data\"))\n",
    "  # 2. Dropout: Only applied on input/projection\n",
    "  # (i.e. x/y in paper and not in internal state/output ct/ht)\n",
    "  dropout_ph = tf.placeholder(tf.float32, name= \"dropout\")\n",
    "\n",
    "  train_inputs = train_data_ph[:num_unrollings]\n",
    "  # labels are inputs shifted by one time step.\n",
    "  train_labels = train_data_ph[1:]  \n",
    "\n",
    "  # Unroll LSTM loop upto num_unrollings times.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state    = saved_state\n",
    "  for i in train_inputs:\n",
    "    # apply dropout at input/embedding layer or output layer:\n",
    "    # i.e. when layers are crossed - not in the recurrent layer\n",
    "    train_embed = tf.nn.dropout(tf.nn.embedding_lookup(embeds, i), dropout_ph)\n",
    "    output, state = lstm_cell(train_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # apply dropout at input/embedding layer or output layer:\n",
    "    # i.e. when layers are crossed - not in the recurrent layer\n",
    "    # Classifier\n",
    "    projection = tf.nn.dropout(tf.concat(0, outputs), dropout_ph)\n",
    "    logits = tf.nn.xw_plus_b(projection, w, b)\n",
    "    labels = tf.concat(0, train_labels)\n",
    "    xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      logits=logits, labels=labels)\n",
    "    loss = tf.reduce_mean(xent)\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1 of charId, no unrolling, and no dropout\n",
    "  sample_input_ph = tf.placeholder(tf.int32, shape=[1], name=\"sample_input\")\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # create a group tensor op to reset sample output/state\n",
    "  reset_sample_state_op = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "\n",
    "  sample_embed = tf.nn.embedding_lookup(embeds, sample_input_ph)\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# num_steps = 100001\n",
    "# summary_frequency = 1000\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.device('/cpu:0'), tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_idbatches.next()\n",
    "    feed_dict = { dropout_ph: 0.90 }\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data_ph[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob_sparse(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          charId = sample_distribution(random_distribution()[0])\n",
    "          sentence = id2char(charId)\n",
    "          reset_sample_state_op.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input_ph: [charId]})\n",
    "            charId = sample_distribution(prediction[0])\n",
    "            sentence += id2char(charId)\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state_op.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_idbatches.next()\n",
    "        predictions = sample_prediction.eval({sample_input_ph: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob_sparse(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Validation set perplexity: 4.73\n",
    "num_nodes = 64; embedding_size = 128; Dropout 0.95\n",
    "Learning Rate 10.0, global_step, 5000, 0.1, staircase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Test Beam Search Implementation. Source: https://github.com/tensorflow/tensorflow/issues/654"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with tf.device('/cpu:0'), graph.as_default():  \n",
    "    beam_size = 3 # Number of hypotheses in beam.\n",
    "    num_symbols = 5 # Output vocabulary size.\n",
    "    embedding_size = 10\n",
    "    num_steps = 3\n",
    "    embedding = tf.zeros([num_symbols, embedding_size])\n",
    "    output_projection = None\n",
    "\n",
    "    # log_beam_probs: list of [beam_size, 1] Tensors\n",
    "    #  Ordered log probabilities of the `beam_size` best hypotheses\n",
    "    #  found in each beam step (highest probability first).\n",
    "    # beam_symbols: list of [beam_size] Tensors \n",
    "    #  The ordered `beam_size` words / symbols extracted by the beam\n",
    "    #  step, which will be appended to their corresponding hypotheses\n",
    "    #  (corresponding hypotheses found in `beam_path`).\n",
    "    # beam_path: list of [beam_size] Tensor\n",
    "    #  The ordered `beam_size` parent indices. Their values range\n",
    "    #  from [0, `beam_size`), and they denote which previous\n",
    "    #  hypothesis each word should be appended to.\n",
    "    log_beam_probs, beam_symbols, beam_path  = [], [], []\n",
    "    def beam_search(prev, i):\n",
    "        # Tensor Dimensions\n",
    "        # Prev: [batch_size x] beam_size x num_symbols\n",
    "        # log_beam_probs[ind]: [batch_size x] beam_size x 1\n",
    "        if output_projection is not None:\n",
    "            prev = tf.nn.xw_plus_b(\n",
    "                prev, output_projection[0], output_projection[1])\n",
    "\n",
    "        # Compute \n",
    "        #  log P(next_word, hypothesis) = \n",
    "        #  log P(next_word | hypothesis)*P(hypothesis) =\n",
    "        #  log P(next_word | hypothesis) + log P(hypothesis)\n",
    "        # for each hypothesis separately, then join them together \n",
    "        # on the same tensor dimension to form the example's \n",
    "        # beam probability distribution:\n",
    "        # [P(word1, hypothesis1), P(word2, hypothesis1), ...,\n",
    "        #  P(word1, hypothesis2), P(word2, hypothesis2), ...]\n",
    "\n",
    "        # If TF had a log_sum_exp operator, then it would be \n",
    "        # more numerically stable to use: \n",
    "        #   probs = prev - tf.log_sum_exp(prev, reduction_dims=[1])\n",
    "        probs = tf.log(tf.nn.softmax(prev))\n",
    "        # i == 1 corresponds to the input being \"<GO>\", with\n",
    "        # uniform prior probability and only the empty hypothesis\n",
    "        # (each row is a separate example).\n",
    "        if i > 1:\n",
    "            probs = tf.reshape(probs + log_beam_probs[-1], \n",
    "                               [-1, beam_size * num_symbols])\n",
    "\n",
    "        # Get the top `beam_size` candidates and reshape them such\n",
    "        # that the number of rows = batch_size * beam_size, which\n",
    "        # allows us to process each hypothesis independently.\n",
    "        ## top_k returns 'k' best_probs and corresponding indices for every row\n",
    "        ## squeeze just swallows all 1 dimensions in a tensor\n",
    "        ## stop_gradient: Stops back propagation: when executed in a graph,\n",
    "        ##                             this op outputs its input tensor as-is and \n",
    "        best_probs, indices = tf.nn.top_k(probs, beam_size)\n",
    "        # indices = tf.stop_gradient(tf.squeeze(tf.reshape(indices, [-1, 1])))\n",
    "        # best_probs = tf.stop_gradient(tf.reshape(best_probs, [-1, 1]))\n",
    "        indices = tf.stop_gradient(tf.squeeze(tf.reshape(indices, [-1, beam_size, 1])))\n",
    "        best_probs = tf.stop_gradient(tf.expand_dims(tf.squeeze(best_probs), -1))\n",
    "\n",
    "        symbols = indices % num_symbols # Which word in vocabulary.\n",
    "        beam_parent = indices // num_symbols # Which hypothesis it came from.\n",
    "\n",
    "        beam_symbols.append(symbols)\n",
    "        beam_path.append(beam_parent)\n",
    "        log_beam_probs.append(best_probs)\n",
    "        return tf.nn.embedding_lookup(embedding, symbols)\n",
    "\n",
    "    # Setting up graph.\n",
    "    inputs = [tf.placeholder(tf.float32, shape=[None, 1, num_symbols])] + [\n",
    "      tf.placeholder(tf.float32, shape=[None, beam_size, num_symbols])\n",
    "      for i in range(num_steps-1)]\n",
    "    for i in range(num_steps):\n",
    "        beam_search(inputs[i], i + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'), tf.Session(graph=graph) as session:\n",
    "    # Running the graph.\n",
    "    input_vals = [0, 0, 0]\n",
    "    l = np.log\n",
    "    eps = -10 # exp(-10) ~= 0\n",
    "    # These values mimic the distribution of vocabulary words\n",
    "    # from each hypothesis independently (in log scale since\n",
    "    # they will be put through exp() in softmax).\n",
    "    input_vals[0] = np.array([0, eps, l(2), eps, l(3)]).reshape(1,1,5)\n",
    "    # Step 1 beam hypotheses =\n",
    "    # (1) Path: [4], prob = log(1 / 2)\n",
    "    # (2) Path: [2], prob = log(1 / 3)\n",
    "    # (3) Path: [0], prob = log(1 / 6)\n",
    "\n",
    "    input_vals[1] = np.array([[l(1.2), 0, 0, l(1.1), 0], # Path [4] \n",
    "                              [0,   eps, eps, eps, eps], # Path [2]\n",
    "                              [0,  0,   0,   0,   0]]) .reshape(1,3,5)  # Path [0]\n",
    "    # Step 2 beam hypotheses =\n",
    "    # (1) Path: [2, 0], prob = log(1 / 3) + log(1)\n",
    "    # (2) Path: [4, 0], prob = log(1 / 2) + log(1.2 / 5.3)\n",
    "    # (3) Path: [4, 3], prob = log(1 / 2) + log(1.1 / 5.3)\n",
    "\n",
    "    input_vals[2] = np.array([[0,  l(1.1), 0,   0,   0], # Path [2, 0]\n",
    "                              [eps, 0,   eps, eps, eps], # Path [4, 0]\n",
    "                              [eps, eps, eps, eps, 0]]).reshape(1,3,5)  # Path [4, 3]\n",
    "    # Step 3 beam hypotheses =\n",
    "    # (1) Path: [4, 0, 1], prob = log(1 / 2) + log(1.2 / 5.3) + log(1)\n",
    "    # (2) Path: [4, 3, 4], prob = log(1 / 2) + log(1.1 / 5.3) + log(1)\n",
    "    # (3) Path: [2, 0, 1], prob = log(1 / 3) + log(1) + log(1.1 / 5.1)\n",
    "\n",
    "    # input_feed = {inputs[i]: input_vals[i][:beam_size, :]\n",
    "    input_feed = {inputs[i]: input_vals[i][:beam_size, :] for i in xrange(num_steps)} \n",
    "    output_feed = beam_symbols + beam_path + log_beam_probs\n",
    "    outputs = session.run(output_feed, feed_dict=input_feed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "expected_beam_symbols = np.array([[4, 2, 0], [0, 0, 3], [1, 4, 1]])\n",
    "expected_beam_path = np.array([[0, 0, 0], [1, 0, 0], [1, 2, 0]])\n",
    "\n",
    "print(\"predicted beam_symbols vs. expected beam_symbols\")\n",
    "for ind, predicted in enumerate(outputs[:num_steps]):\n",
    "  print(\"(\", predicted, \",\", expected_beam_symbols[ind], \")\")\n",
    "\n",
    "print(\"\\npredicted beam_path vs. expected beam_path\")\n",
    "for ind, predicted in enumerate(outputs[num_steps:num_steps * 2]):\n",
    "  print(\"(\", predicted, \",\", expected_beam_path[ind], \")\")\n",
    "\n",
    "print(\"\\nlog beam probs\")\n",
    "for log_probs in outputs[2 * num_steps:]:\n",
    "  print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'), tf.Session(graph=graph) as session:\n",
    "    # Running the graph.\n",
    "    l = np.log\n",
    "    eps = -10 # exp(-10) ~= 0\n",
    "    # These values mimic the distribution of vocabulary words\n",
    "    # from each hypothesis independently (in log scale since\n",
    "    # they will be put through exp() in softmax).\n",
    "    inp_vals = np.array([0, eps, l(2), eps, l(3)])\n",
    "    input_vals[0] = np.vstack((inp_vals, inp_vals)).reshape(2,1,5)\n",
    "    # Step 1 beam hypotheses =\n",
    "    # (1) Path: [4], prob = log(1 / 2)\n",
    "    # (2) Path: [2], prob = log(1 / 3)\n",
    "    # (3) Path: [0], prob = log(1 / 6)\n",
    "\n",
    "    inp_vals = np.array([[l(1.2), 0, 0, l(1.1), 0], # Path [4] \n",
    "                              [0,   eps, eps, eps, eps], # Path [2]\n",
    "                              [0,  0,   0,   0,   0]])   # Path [0]\n",
    "    input_vals[1] = np.vstack((inp_vals, inp_vals)).reshape(2,3,5)\n",
    "    # Step 2 beam hypotheses =\n",
    "    # (1) Path: [2, 0], prob = log(1 / 3) + log(1)\n",
    "    # (2) Path: [4, 0], prob = log(1 / 2) + log(1.2 / 5.3)\n",
    "    # (3) Path: [4, 3], prob = log(1 / 2) + log(1.1 / 5.3)\n",
    "\n",
    "    inp_vals = np.array([[0,  l(1.1), 0,   0,   0], # Path [2, 0]\n",
    "                         [eps, 0,   eps, eps, eps], # Path [4, 0]\n",
    "                         [eps, eps, eps, eps, 0]])  # Path [4, 3]\n",
    "    input_vals[2] = np.vstack((inp_vals,inp_vals)).reshape(2,3,5)\n",
    "    # Step 3 beam hypotheses =\n",
    "    # (1) Path: [4, 0, 1], prob = log(1 / 2) + log(1.2 / 5.3) + log(1)\n",
    "    # (2) Path: [4, 3, 4], prob = log(1 / 2) + log(1.1 / 5.3) + log(1)\n",
    "    # (3) Path: [2, 0, 1], prob = log(1 / 3) + log(1) + log(1.1 / 5.1)\n",
    "\n",
    "    input_feed = {inputs[i]: input_vals[i] for i in xrange(num_steps)} \n",
    "    output_feed = beam_symbols + beam_path + log_beam_probs\n",
    "    outputs = session.run(output_feed, feed_dict=input_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "expected_beam_symbols = np.array([[[4, 2, 0],[4, 2, 0],], [[0, 0, 3],[0, 0, 3],], [[1, 4, 1],[1, 4, 1]]])\n",
    "expected_beam_path = np.array([[[0, 0, 0],[0, 0, 0],], [[1, 0, 0],[1, 0, 0],], [[1, 2, 0],[1, 2, 0]]])\n",
    "\n",
    "print(\"predicted beam_symbols vs. expected beam_symbols\")\n",
    "for ind, predicted in enumerate(outputs[:num_steps]):\n",
    "  print(\"(\", predicted, \",\", expected_beam_symbols[ind], \")\")\n",
    "\n",
    "print(\"\\npredicted beam_path vs. expected beam_path\")\n",
    "for ind, predicted in enumerate(outputs[num_steps:num_steps * 2]):\n",
    "  print(\"(\", predicted, \",\", expected_beam_path[ind], \")\")\n",
    "\n",
    "print(\"\\nlog beam probs\")\n",
    "for log_probs in outputs[2 * num_steps:]:\n",
    "  print(log_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/\n",
    "Magic Commands\n",
    "> %lsmagic\n",
    "> %env: sets environment variables\n",
    "> %run: other python file or ipython NB\n",
    "> %time %timeit: measure slow code\n",
    "> %prun: measures how much time program spent in each function\n",
    "> %pdb: automatically turns on python debugging\n",
    "> !cmd: executes shell commands\n",
    "Latex Commands directly inserted in Markdown cell\n",
    "$$ P(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)} $$\n",
    ">%%python3: executes cell in python3 kernel\n",
    ">%%bash: executes cell in bash kernel\n",
    "RISE allows create a powerpoint style presentation from existing notebook\n",
    "$> pip install RISE\n",
    "$> jupyter-nbextension install rise --py --sys-prefix\n",
    "$> jupyter-nbextension enable rise --py --sys-prefix"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": {},
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "name": "6_lstm.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
