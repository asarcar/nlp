{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_root = './data'\n",
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "print(\"{}\".format(pickle_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Randomly picks a batch of training and validation data\n",
    "def get_data(self, dataset, labels):\n",
    "  n = len(dataset)\n",
    "  perm = np.random.randint(n, size=self.batch_size)\n",
    "  return dataset[perm], labels[perm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "LOGISTIC REGRESSION: Softmax Score = X*W1 + B1\n",
    "NEURAL MODEL: Softmax Score =(ReLU(X*W1 + B1))*W2 + B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# LOGISTIC REGRESSION: Softmax Score = X*W1 + B1\n",
    "def define_state_logistic(self):\n",
    "  with self.graph.as_default():\n",
    "    # Input Layer placeholders for data and labels\n",
    "    self.dataset_ph = tf.placeholder(tf.float32,\n",
    "                                     shape=(None, self.n_features), name=\"dataset\")\n",
    "    self.labels_ph = tf.placeholder(tf.float32,\n",
    "                                    shape=(None, self.label_size), name=\"labels\")\n",
    "    self.l2_reg_ph = tf.placeholder(tf.float32, name= \"l2_reg\")\n",
    "    # Use truncated normal initializer\n",
    "    tr_norm_init = tf.truncated_normal_initializer()\n",
    "\n",
    "    # Softmax Score Layer:\n",
    "    with tf.variable_scope(\"Softmax\", initializer=tr_norm_init) as scope:\n",
    "      sm_weights = tf.get_variable(\"Weights\", [self.n_features, self.label_size])\n",
    "      sm_biases    = tf.get_variable(\"Biases\", [self.label_size], initializer=tf.zeros_initializer)\n",
    "\n",
    "    return\n",
    "def define_computation_logistic(self):\n",
    "  with self.graph.as_default():\n",
    "    # Input/Placeholder to NN\n",
    "    with tf.variable_scope(\"Softmax\", reuse=True) as scope:\n",
    "      sm_weights = tf.get_variable(\"Weights\")\n",
    "      sm_biases = tf.get_variable(\"Biases\")\n",
    "\n",
    "    logits = tf.matmul(self.dataset_ph, sm_weights) + sm_biases\n",
    "\n",
    "    # SoftmaxScore to Prediction\n",
    "    self.prediction_op = tf.nn.softmax(logits, name=\"Prediction\")\n",
    "    # Cross Entropy Loss\n",
    "    XEnt = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=self.labels_ph)\n",
    "  \n",
    "    # Scalar Loss\n",
    "    self.loss_op = tf.reduce_mean(XEnt) + self.l2_reg_ph*tf.nn.l2_loss(sm_weights)\n",
    "  \n",
    "    # Accuracy\n",
    "    P = tf.argmax(self.prediction_op, axis=1)\n",
    "    L = tf.argmax(self.labels_ph, axis=1)\n",
    "    self.accuracy_op = 100.0 * tf.reduce_mean(tf.to_float(tf.equal(P,L)))\n",
    "  \n",
    "    # Optimizer\n",
    "    self.train_op = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss_op)\n",
    "\n",
    "    return\n",
    "\n",
    "def run_epochs(self):\n",
    "  best_val_epoch = 0\n",
    "  best_val_acc      = -float('inf')\n",
    "\n",
    "  with tf.device('/cpu:0'), tf.Session(graph=self.graph) as s:\n",
    "    initer = tf.global_variables_initializer()\n",
    "    s.run(initer)\n",
    "    print(\"Initialized\")\n",
    "\n",
    "    for step in range(self.n_epochs):\n",
    "      # Generate a minibatch.\n",
    "      batch_data, batch_labels = self.get_data(train_dataset, train_labels)\n",
    "      # Dictionary: Feeds Training Minibatch.\n",
    "      feed_dict = {self.dataset_ph: batch_data, self.labels_ph: batch_labels, self.l2_reg_ph:self.l2_reg}\n",
    "      _, tr_loss, tr_acc = s.run( [self.train_op, self.loss_op, self.accuracy_op], feed_dict=feed_dict)\n",
    "\n",
    "      if ((step != 0) and (step % 500 == 0)):\n",
    "        # Track validationation accuracy\n",
    "        feed_dict = {self.dataset_ph: valid_dataset, self.labels_ph: valid_labels, self.l2_reg_ph:1.0}\n",
    "        val_acc = s.run(self.accuracy_op, feed_dict=feed_dict)\n",
    "\n",
    "        print(\"Minibatch Step=%d\"%step)\n",
    "        print(\"------------------------------\")\n",
    "        print(\"Training Loss=%.2f: Accuracy=%.2f%%\" % (tr_loss, tr_acc))\n",
    "        print(\"Validation Accuracy: %.2f%%\" % val_acc)\n",
    "      \n",
    "        # Remember the epoch when best validation accuracy was realized\n",
    "        # Stop early if average validation accuracy is not improving for a few steps\n",
    "        if val_acc > best_val_acc:\n",
    "          best_val_epoch = step\n",
    "          best_val_acc = val_acc\n",
    "          \n",
    "        if step >= (best_val_epoch + self.early_stop):\n",
    "          print(\"Step %d: Best_Epoch %d: Early_Stop %d\" % (step, best_val_epoch, self.early_stop))\n",
    "          print(\"Terminate training early. Validation accuracy has flattened at %.2f%%\"% best_val_acc)\n",
    "          break\n",
    "      \n",
    "    print(\"Training Completed: \")\n",
    "    print(\"-------------------------------\")\n",
    "    feed_dict = {self.dataset_ph: test_dataset, self.labels_ph: test_labels, self.l2_reg_ph:1.0}\n",
    "    test_acc = s.run( self.accuracy_op, feed_dict=feed_dict)\n",
    "    print(\"Test Accuracy: %.2f%%\" % test_acc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "# image_size = 28\n",
    "# num_labels = 10\n",
    "# hidden layer\n",
    "class ImageModel():\n",
    "  l2_reg           = 0.01\n",
    "  lr                    = 0.01\n",
    "  label_size    = num_labels\n",
    "  n_features   = image_size*image_size\n",
    "  batch_size   = 128\n",
    "  n_epochs     = 100000\n",
    "  early_stop   = 1000\n",
    "\n",
    "  define_state = define_state_logistic\n",
    "  define_computation = define_computation_logistic\n",
    "  get_data = get_data\n",
    "  run_epochs = run_epochs  \n",
    "\n",
    "  def __init__(self):\n",
    "    self.graph = tf.Graph()\n",
    "    \n",
    "imMod = ImageModel()    \n",
    "imMod.define_state()\n",
    "imMod.define_computation()\n",
    "imMod.run_epochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "# image_size = 28\n",
    "# num_labels = 10\n",
    "# hidden layer\n",
    "class ImageModel():\n",
    "  l2_reg           = 0.01\n",
    "  dropout       = 1.0\n",
    "  lr                    = 0.01\n",
    "  label_size    = num_labels\n",
    "  hidden_size = 1024\n",
    "  n_features   = image_size*image_size\n",
    "  batch_size   = 128\n",
    "  n_epochs     = 100000\n",
    "  early_stop   = 1000\n",
    "\n",
    "  define_state = define_state_nn\n",
    "  define_computation = define_computation_nn\n",
    "  get_data = get_data\n",
    "  run_epochs = run_epochs  \n",
    "\n",
    "  def __init__(self):\n",
    "    self.graph = tf.Graph()\n",
    "    \n",
    "imMod = ImageModel()    \n",
    "imMod.define_state()\n",
    "imMod.define_computation()\n",
    "imMod.run_epochs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "# image_size = 28\n",
    "# num_labels = 10\n",
    "# hidden layer\n",
    "class ImageModel():\n",
    "  l2_reg           = 0.01\n",
    "  dropout       = 1.0\n",
    "  lr                    = 0.01\n",
    "  label_size    = num_labels\n",
    "  hidden_size = 1024\n",
    "  n_features   = image_size*image_size\n",
    "  batch_size   = 128\n",
    "  n_epochs     = 1000\n",
    "  early_stop   = 1000\n",
    "\n",
    "  define_state = define_state_nn\n",
    "  define_computation = define_computation_nn\n",
    "  get_data = get_data\n",
    "  run_epochs = run_epochs  \n",
    "\n",
    "  def __init__(self):\n",
    "    self.graph = tf.Graph()\n",
    "    \n",
    "imMod = ImageModel()    \n",
    "imMod.define_state()\n",
    "imMod.define_computation()\n",
    "imMod.run_epochs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "# image_size = 28\n",
    "# num_labels = 10\n",
    "# hidden layer\n",
    "class ImageModel():\n",
    "  l2_reg           = 0.01\n",
    "  dropout       = 0.9\n",
    "  lr                    = 0.01\n",
    "  label_size    = num_labels\n",
    "  hidden_size = 1024\n",
    "  n_features   = image_size*image_size\n",
    "  batch_size   = 128\n",
    "  n_epochs     = 1000\n",
    "  early_stop   = 1000\n",
    "\n",
    "  define_state = define_state_nn\n",
    "  define_computation = define_computation_nn\n",
    "  get_data = get_data\n",
    "  run_epochs = run_epochs  \n",
    "\n",
    "  def __init__(self):\n",
    "    self.graph = tf.Graph()\n",
    "    \n",
    "imMod = ImageModel()    \n",
    "imMod.define_state()\n",
    "imMod.define_computation()\n",
    "imMod.run_epochs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# DEEP NEURAL MODEL: Softmax Score =(((ReLU(X*W1 + B1))*W2 + B2)*W3 + B3)*W4 + B4\n",
    "def define_state_deep_nn(self):\n",
    "  with self.graph.as_default():\n",
    "    # Input Layer placeholders for data and labels\n",
    "    self.dataset_ph = tf.placeholder(tf.float32,\n",
    "                                     shape=(None, self.n_features), name=\"dataset\")\n",
    "    self.labels_ph = tf.placeholder(tf.float32,\n",
    "                                    shape=(None, self.label_size), name=\"labels\")\n",
    "    self.l2_reg_ph = tf.placeholder(tf.float32, name= \"l2_reg\")\n",
    "    self.dropout_ph = tf.placeholder(tf.float32, name= \"dropout\")\n",
    "\n",
    "    # Use truncated normal initializer\n",
    "    tr_norm_init = tf.truncated_normal_initializer(stddev=0.1)\n",
    "    # NN Layer 1:\n",
    "    with tf.variable_scope(\"NN1\", initializer=tr_norm_init) as scope:\n",
    "      nn1_weights = tf.get_variable(\"Weights\", [self.n_features, self.hidden1_size])\n",
    "      nn1_biases    = tf.get_variable(\"Biases\", [self.hidden1_size], initializer=tf.zeros_initializer)\n",
    "    # NN Layer 2:\n",
    "    with tf.variable_scope(\"NN2\", initializer=tr_norm_init) as scope:\n",
    "      nn2_weights = tf.get_variable(\"Weights\", [self.hidden1_size, self.hidden2_size])\n",
    "      nn2_biases    = tf.get_variable(\"Biases\", [self.hidden2_size], initializer=tf.zeros_initializer)\n",
    "    # NN Layer 3:\n",
    "    with tf.variable_scope(\"NN3\", initializer=tr_norm_init) as scope:\n",
    "      nn2_weights = tf.get_variable(\"Weights\", [self.hidden2_size, self.hidden3_size])\n",
    "      nn2_biases    = tf.get_variable(\"Biases\", [self.hidden3_size], initializer=tf.zeros_initializer)\n",
    "    # Softmax Score Layer:\n",
    "    with tf.variable_scope(\"Softmax\", initializer=tr_norm_init) as scope:\n",
    "      sm_weights = tf.get_variable(\"Weights\", [self.hidden3_size, self.label_size])\n",
    "      sm_biases    = tf.get_variable(\"Biases\", [self.label_size], initializer=tf.zeros_initializer)\n",
    "\n",
    "    return\n",
    "    \n",
    "def define_computation_deep_nn(self):\n",
    "  with self.graph.as_default():\n",
    "    # NN1\n",
    "    with tf.variable_scope(\"NN1\", reuse=True) as scope:\n",
    "      nn1_weights = tf.get_variable(\"Weights\")\n",
    "      nn1_biases = tf.get_variable(\"Biases\")\n",
    "\n",
    "    Z1 = tf.matmul(self.dataset_ph, nn1_weights) + nn1_biases\n",
    "    A1 = tf.nn.relu(Z1, \"ReLU\")\n",
    "    # Dropout at Activation\n",
    "    Ad1 = tf.nn.dropout(A1, self.dropout_ph)\n",
    "    nn1_loss = tf.nn.l2_loss(nn1_weights)\n",
    "\n",
    "    # NN2\n",
    "    with tf.variable_scope(\"NN2\", reuse=True) as scope:\n",
    "       nn2_weights = tf.get_variable(\"Weights\")\n",
    "       nn2_biases = tf.get_variable(\"Biases\")\n",
    "\n",
    "    Z2 = tf.matmul(Ad1, nn2_weights) + nn2_biases\n",
    "    A2 = tf.nn.relu(Z2, \"ReLU\")\n",
    "    # Dropout at Activation\n",
    "    Ad2 = tf.nn.dropout(A2, self.dropout_ph)\n",
    "    nn2_loss = tf.nn.l2_loss(nn2_weights)\n",
    "  \n",
    "    # NN3\n",
    "    with tf.variable_scope(\"NN2\", reuse=True) as scope:\n",
    "       nn3_weights = tf.get_variable(\"Weights\")\n",
    "       nn3_biases = tf.get_variable(\"Biases\")\n",
    "\n",
    "    Z3 = tf.matmul(Ad2, nn3_weights) + nn3_biases\n",
    "    A3 = tf.nn.relu(Z3, \"ReLU\")\n",
    "    # Dropout at Activation\n",
    "    Ad3 = tf.nn.dropout(A3, self.dropout_ph)\n",
    "    nn3_loss = tf.nn.l2_loss(nn3_weights)\n",
    "  \n",
    "    # NN3 to SoftmaxScore\n",
    "    with tf.variable_scope(\"Softmax\", reuse=True) as scope:\n",
    "      sm_weights = tf.get_variable(\"Weights\")\n",
    "      sm_biases    = tf.get_variable(\"Biases\")\n",
    "    Z4 = tf.matmul(Ad3, sm_weights) + sm_biases\n",
    "    # Dropout at Output\n",
    "    logits = tf.nn.dropout(Z4, self.dropout_ph)  \n",
    "    sm_loss = tf.nn.l2_loss(sm_weights)\n",
    "    \n",
    "    # SoftmaxScore to Prediction\n",
    "    self.prediction_op = tf.nn.softmax(logits, name=\"Prediction\")\n",
    "\n",
    "    # Cross Entropy Loss\n",
    "    XEnt = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=self.labels_ph)\n",
    "  \n",
    "    # Scalar Loss\n",
    "    self.loss_op = tf.reduce_mean(XEnt) + self.l2_reg_ph*(nn1_loss + nn2_loss + nn3_loss + sm_loss)\n",
    "  \n",
    "    # Accuracy\n",
    "    P = tf.argmax(self.prediction_op, axis=1)\n",
    "    L = tf.argmax(self.labels_ph, axis=1)\n",
    "    self.accuracy_op = 100.0 * tf.reduce_mean(tf.to_float(tf.equal(P,L)))\n",
    "\n",
    "    # Optimizer with exponential decay of learning rate\n",
    "    n_steps = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(self.lr, n_steps, self.decay_steps, self.decay_rate)\n",
    "    self.train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss_op, global_step=n_steps)\n",
    "\n",
    "  return\n",
    "\n",
    "def run_epochs_deep_nn(self):\n",
    "  best_val_epoch = 0\n",
    "  best_val_acc      = -float('inf')\n",
    "\n",
    "  with tf.device('/cpu:0'), tf.Session(graph=self.graph) as s:\n",
    "    initer = tf.global_variables_initializer()\n",
    "    s.run(initer)\n",
    "    print(\"Initialized\")\n",
    "\n",
    "    for step in range(self.n_epochs):\n",
    "      # Generate a minibatch.\n",
    "      batch_data, batch_labels = self.get_data(train_dataset, train_labels)\n",
    "      # Dictionary: Feeds Training Minibatch.\n",
    "      feed_dict = {\n",
    "        self.dataset_ph:    batch_data,\n",
    "        self.labels_ph:       batch_labels,\n",
    "        self.l2_reg_ph:       self.l2_reg,\n",
    "        self.dropout_ph:   self.dropout\n",
    "      }\n",
    "      _, tr_loss, tr_acc = s.run( [self.train_op, self.loss_op, self.accuracy_op], feed_dict=feed_dict)\n",
    "\n",
    "      if (step % 500 == 0):\n",
    "        # Track validationation accuracy\n",
    "        feed_dict = {\n",
    "          self.dataset_ph: valid_dataset,\n",
    "          self.labels_ph: valid_labels,\n",
    "          self.l2_reg_ph:0.0,\n",
    "          self.dropout_ph:1.0\n",
    "        }\n",
    "        val_acc = s.run(self.accuracy_op, feed_dict=feed_dict)\n",
    "\n",
    "        print(\"Minibatch Step=%d\"%step)\n",
    "        print(\"------------------------------\")\n",
    "        print(\"Training Loss=%.2f: Accuracy=%.2f%%\" % (tr_loss, tr_acc))\n",
    "        print(\"Validation Accuracy: %.2f%%\" % val_acc)\n",
    "      \n",
    "        # Remember the epoch when best validation accuracy was realized\n",
    "        # Stop early if average validation accuracy is not improving for a few steps\n",
    "        if val_acc > best_val_acc:\n",
    "          best_val_epoch = step\n",
    "          best_val_acc = val_acc\n",
    "          \n",
    "        if step >= (best_val_epoch + self.early_stop):\n",
    "          print(\"Step %d: Best_Epoch %d: Early_Stop %d\" % (step, best_val_epoch, self.early_stop))\n",
    "          print(\"Terminate training early. Validation accuracy has flattened at %.2f%%\"% best_val_acc)\n",
    "          break\n",
    "      \n",
    "    print(\"Training Completed: \")\n",
    "    print(\"-------------------------------\")\n",
    "    feed_dict = {\n",
    "      self.dataset_ph: test_dataset,\n",
    "      self.labels_ph: test_labels,\n",
    "      self.l2_reg_ph:0.0,\n",
    "      self.dropout_ph:1.0\n",
    "    }\n",
    "    test_acc = s.run( self.accuracy_op, feed_dict=feed_dict)\n",
    "    print(\"Test Accuracy: %.2f%%\" % test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "# image_size = 28\n",
    "# num_labels = 10\n",
    "# hidden layer\n",
    "class ImageModel():\n",
    "  l2_reg           = 0.01\n",
    "  dropout       = 0.9\n",
    "  lr                    = 0.1\n",
    "  decay_rate   = 0.95\n",
    "  decay_steps = 100\n",
    "  label_size    = num_labels\n",
    "  hidden1_size = 1024\n",
    "  hidden2_size = 1024\n",
    "  hidden3_size = 1024\n",
    "  n_features   = image_size*image_size\n",
    "  batch_size   = 128\n",
    "  n_epochs     = 10000\n",
    "  early_stop   = 2500\n",
    "\n",
    "  define_state = define_state_deep_nn\n",
    "  define_computation = define_computation_deep_nn\n",
    "  get_data = get_data\n",
    "  run_epochs = run_epochs_deep_nn\n",
    "\n",
    "  def __init__(self):\n",
    "    self.graph = tf.Graph()\n",
    "    \n",
    "imMod = ImageModel()    \n",
    "imMod.define_state()\n",
    "imMod.define_computation()\n",
    "imMod.run_epochs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Accuracy of around 91.49% was accomplished with the following hyperparameters:\n",
    "* 2 layer NN of 512 units each.\n",
    "* L2 Reg 0.01 Dropout 0.95\n",
    "* LR 0.1 Decay Rate 0.90 Decay Steps 1000\n",
    "Accuracy of around 90.51% was accomplished with the following hyperparameters:\n",
    "* 3 layer NN of 1024 units each.\n",
    "* L2 Reg 0.01 Dropout 0.9\n",
    "* LR 0.1 Decay Rate 0.95 Decay Steps 100\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": {},
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "name": "3_regularization.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
